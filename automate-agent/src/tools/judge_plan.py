""" Judge Generated Plan Tool """
import logging
from pydantic import BaseModel
from utils.inference_gateway import InferenceGateway
from utils.settings import settings

logger = logging.getLogger(__name__)

MAX_RETRIES = 3

SYSTEM_PROMPT = """
    You are an AI Agent whose role is to quality check the implementation plan generated by a competing
    model and confirm that the plan adequately and comrehensively fulfills the user's automation request.
    You will be provided the user's original request as well as IT infrastructure content that describes
    the user's environment for which the automation must cooperate within.

    Your response must be formatted in JSON and include an overall evaluation of the plan, as well as 
    enhancement or modification suggestions, if any.  The competing model will be asked to adjust its 
    plan based on your guidance.

    Output Format:
    {
        "feedback": ["FEEDBACK_1", "FEEDBACK_2", ...],
        "revise_plan_flag": false
    }

    Example #1
    {
        "feedback": ["This is a detailed and thorough plan that is well thought out."],
        "revise_plan_flag": false
    }

    Example #2
    {
        "feedback": [
            "This plan is too detailed for such a simple task.",
            "Delete tasks 1 and 3 as those commands are always included with the operating system.",
        ],
        "revise_plan_flag": true
    }

    Example #3
    {
        "feedback": [
            "Ensure the hypervisor host has free CPUs available before increasing VM size",
            "Validate that the input filename exists before starting work",
            "If an error occurs while applying the change, revert the file update"
        ],
        "revise_plan_flag": true
    }
"""

class JudgePlanResponse(BaseModel):
    feedback: list[str]
    revise_plan_flag: bool

async def judge_plan(user_request: str, research: str, nominated_plan: str) -> JudgePlanResponse:
    """ (Step 3 of 6)  Review the provided implementation plan for an automation generation request for
        accuracy and quality.  The result will be suggestions and criticism, if any, that must be 
        incorporated into the implementation plan.  If the suggested plan is modified based on the
        feedback, it must be re-judged by reinvoking this tool.
    
        user_request - (required) write up for the user's request for automation
        research - (required) helpful research as it relates to implementing the user's automation request
        suggested_plan - nominated implementation plan for the request

        Returns: Quality assessment of the provided parameteres
    """
    logger.info("judge_plan parameters.  User_Request=%s  Research=%s. Nominated_Plan=%s",\
                user_request, research, nominated_plan)

    # validate that all required parameters were provided
    if user_request is None or len(user_request) == 0:
        msg = "ERROR: 'user_request' is a required argument and cannot be empty"
        logger.error(msg)
        return msg
    if research is None or len(research) == 0:
        msg = "ERROR: 'research' is a required argument and cannot be empty"
        logger.error(msg)
        return msg
    if nominated_plan is None or len(nominated_plan) == 0:
        msg = "ERROR: 'nominated_plan' is a required argument and cannot be empty"
        logger.error(msg)
        return msg

    # build user prompt
    prompt = f"""Implementation Plan to Judge:
        {nominated_plan}

        User Request:
        {user_request}

        Context:
        {research}"""

    # execute inference
    gateway = InferenceGateway()
    retry_count = 0
    while (retry_count < MAX_RETRIES):
        critical_feedback = gateway.json_chat(SYSTEM_PROMPT, prompt, settings.JUDGE_PLAN_MODEL)
        logger.info("Critical Feedback (Unqualified): %s", critical_feedback)
    
        # parse response
        if "revise_plan_flag" in critical_feedback and \
           "feedback" in critical_feedback:
            revise_plan_flag = critical_feedback["revise_plan_flag"]
            feedback = critical_feedback["feedback"]
            return JudgePlanResponse(feedback=feedback, revise_plan_flag=revise_plan_flag)

        retry_count += 1

    # raise exception
    msg = "Unable to build a parsable judge response for the user request.  Kept returning bad input."
    logger.error(msg)
    raise ValueError(msg)
    